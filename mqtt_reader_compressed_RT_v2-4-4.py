from canlib import canlib, Frame
import time
import socket
import threading
from awscrt import io, mqtt, auth, http
from awsiot import mqtt_connection_builder
import time as t
import json
import cantools
import datetime
from copy import deepcopy
from operator import itemgetter
import os
from queue import Queue

#################### README ###############################
# Fonction : ce programme est un reader du VCU
#   1. Se connecte au brocker MQTT d'AWS dans son initialisation
#   Puis dans 3 threads :
#       a. Lis les messages MQTT auqels il a subscribe,
#          il d√©code, d√©concat√®ne et d√©compresse (via dbc) tout sauf la partie datades trames (UDH),
#          puis les rend disponibles dans un tableau pour la partie b.
#       b. R√©cup√®re les datas UDH √† intervalle de temps constant et avec un retard,
#          puis les d√©compresse et les mets dans l'orde et dans une fifo pour le c.
#       c. Cr√©er et g√®re une connexion TCP, attend la connexion de Labview et lui
#          envoi les datas lu dans la fifo.
#   2. Toutes les fonctions avec "save_" sont des fonctions de debug pour les tests,
#      elles permettent de sauvegarder sous le format .trc ou .json.
###########################################################

#################### Abr√©viation ##########################
# UDH = Unconcatene, Decompresse id mais pas data -> Half
###########################################################

#Class permettant de regrouper tous les √©l√©ments d'une trame
class TrameDBC:
    def __init__(self, name, id_dec, id_hexa, dlc, ext,
                 id_compressed, can_index,cycle):
        self.name = name
        self.id = id_dec
        self.dlc= dlc
        self.ext = ext
        self.id_compressed = id_compressed
        self.id_hexa = id_hexa
        self.can_index = can_index
        self.cycle= cycle

#Class permettant de stocker toutes les objets de Trame
#c'est un handler 
class TramesDBC:
    def __init__(self):
        self.trames = []
    
    def add_trame(self,trame):
        self.trames.append(trame)

# Save on .TRC format le tbaleau contenu dans history_msg_UDH
def save_trc():
    global history_msg_UDH
    #debug 
    global msg_uncompress_history_sorted
    print("TRC saving ‚åö") 

    #--------------Tri--------------
    #tri les trames dans l'odre chronologique en focntion du token
    msg_history_sorted = sorted(history_msg_UDH,key=itemgetter('tri'))
    msg_uncompress_history_sorted = uncompress_mqtt_msg(msg_history_sorted)
    #msg_uncompress_history_sorted = uncompress_mqtt_msg(history_msg_UDH) # Desactive le tri
    # Ajoute les datas en hexa
    for msg in msg_uncompress_history_sorted:
        msg["hex"] = [hex(entier) for entier in msg["data"]]

    ##---------------------------------------------------------------------------------------
    
    #ici on rend compatible la partie d√©compression avec le reste du programme
    frames = msg_uncompress_history_sorted#msg_uncompressed_history

    #mise en forme de la start_date
    start_date = datetime.datetime.fromtimestamp(frames[0]['start'])
    start_date = start_date.strftime("%d/%m/%Y %H:%M:%S")

    #------------Benchmark data out ---------------------------------------------------
    # nb_octet_uncompressed = 0
    # for data_uncompressed in list_uncompressed_sorted:
    #         nb_octet_uncompressed += 1 + 2  + 1  + len(data_uncompressed["data"])
    #         #                     id +time+sort+ data
    # print ("facteur compression : " + str(nb_octet_uncompressed/nb_octet_compressed))
    #----------------------------------------------------------------------------------

    #ecriture fichier de sortie
    i = 0

    # cr√©ation nom du fichier
    now = datetime.datetime.now()
    now_str = now.strftime("%d-%m-%Y_%H-%M-%S")
    file_name = "uncompress_" + now_str + ".trc"
    with open(file_name, 'w') as f: 
    
        #Ecriture en t√™te
        f.write(""";$FILEVERSION=1.3
;$STARTTIME=44956.551155115827
;
;   Start time: """ + str(start_date) +"""
;   Generated by NWT Network Bridge v1.0.0
;-------------------------------------------------------------------------------
;   Bus  Connection   Net Connection     Protocol  Bit rate
;   1    Connection1  Mqtt               Custom    Custom
;-------------------------------------------------------------------------------
;   Message   Time    Bus  Type   ID    Reserved
;   Number    Offset  |    |      [hex] |   Data Length Code
;   |         [ms]    |    |      |     |   |    Data [hex] ...
;   |         |       |    |      |     |   |    |
;---+-- ------+------ +- --+-- ---+---- +- -+-- -+ -- -- -- -- -- -- --""")
        for trame in frames:
            #Mise en forme id
            id = str(trame["id"])
            #Conversion en string hex
            id = str(hex(int(id))) 
            #Enleve le "0x"
            id = id.replace("0x","")
            #Met les lettres en majuscule
            id = id.upper()
            #les id can doivent sur 4 digit minimum pour les id classiques
            if len(id) < 4 and trame["ext"] == '0':
                id = id.zfill(4)
            #les id can doivent sur 8 digit minimum pour les id extend
            if  trame["ext"] == 1:
                id = id.zfill(8)
            #conversion time
            time_ms = trame["time_ms"].split('.')[0]
            #car pas de us dans les data re√ßu par le log
            if "." in trame["time_ms"]:
                us = trame["time_ms"].split('.')[1]
            else :
                us = str(0)
            time_s = trame["time_s"]#[-5:] #garde uniquement les 5 premiers caracteres car sinon trop long
            #les ms et us doivent etre sur 3 digits
            if len(us) < 3:
                us = us.ljust(3,'0')
            if len(time_ms) < 3:
                time_ms = time_ms.zfill(3)
            #Pour avoir l'offset par rapport au d√©but de l'enregistrement
            #trame["start"] = 1768843635 #-------------test
            ms_int = int(time_s) - int(trame["start"])
            ms = str(ms_int) + time_ms #concatene le temps en s avec le temps en ms pour epochtime en ms
            
            #les data doivent √™tre en hexa
            data = []  #data2 pour inviter interference avec autres fonctions
            for j in range (0,trame['dlc']):
                data.append(str(hex(trame["data"][j])))
                data[j] = data[j].replace("0x","")
                data[j] = data[j].upper()
                if len(data[j]) < 2:
                    data[j] = data[j].zfill(2)
            
            #Mise en forme data en fonction du DLC
            data_to_write =""
            for k in range (0,trame["dlc"]):
                data_to_write = data_to_write + data[k] + " "

            #ecriture data
            f.write("\n" + (6-len(str(i+1)))*" " + str(i+1) + ")" +(10-len(ms))*" " + ms
            + "."+ us + " " + "1" + 2*" " + "Rx" + (12-len(id)) * " " 
            + id + " " + "_" + 2*" " + str(trame["dlc"]) + 4*" " + data_to_write)
            #+ " Data_sorter: " + trame["data_sorter"])  #Only for debug
            i+=1
    print("TRC saved ‚úÖ")

# Save on .trc a struct for debug
def save_trc_debug(to_save):
    # To save directly debug_history
    # global debug_history
    # to_save = debug_history
    ##---------------------------------------------------------------------------------------
    frames =[]
    #Enl√®ve l'indicateur de queue
    for frame in to_save:
        if frame !="NEW PACKET" and frame !=[]:
            frames.append(frame)#msg_uncompressed_history

    #mise en forme de la start_date
    start_date = datetime.datetime.fromtimestamp(frames[0]['start'])
    start_date = start_date.strftime("%d/%m/%Y %H:%M:%S")

    #------------Benchmark data out ---------------------------------------------------
    # nb_octet_uncompressed = 0
    # for data_uncompressed in list_uncompressed_sorted:
    #         nb_octet_uncompressed += 1 + 2  + 1  + len(data_uncompressed["data"])
    #         #                     id +time+sort+ data
    # print ("facteur compression : " + str(nb_octet_uncompressed/nb_octet_compressed))
    #----------------------------------------------------------------------------------

    #ecriture fichier de sortie
    i = 0
    # cr√©ation nom du fichier
    now = datetime.datetime.now()
    now_str = now.strftime("%d-%m-%Y_%H-%M-%S")
    file_name = "debug_queue_" + now_str + ".trc"
    with open(file_name, 'w') as f: 
    
        #Ecriture en t√™te
        f.write(""";$FILEVERSION=1.3
;$STARTTIME=44956.551155115827
;
;   Start time: """ + str(start_date) +"""
;   Generated by NWT Network Bridge v1.0.0
;-------------------------------------------------------------------------------
;   Bus  Connection   Net Connection     Protocol  Bit rate
;   1    Connection1  Mqtt               Custom    Custom
;-------------------------------------------------------------------------------
;   Message   Time    Bus  Type   ID    Reserved
;   Number    Offset  |    |      [hex] |   Data Length Code
;   |         [ms]    |    |      |     |   |    Data [hex] ...
;   |         |       |    |      |     |   |    |
;---+-- ------+------ +- --+-- ---+---- +- -+-- -+ -- -- -- -- -- -- --""")
        for trame in frames:
            #Mise en forme id
            id = str(trame["id"])
            #Conversion en string hex
            id = str(hex(int(id))) 
            #Enleve le "0x"
            id = id.replace("0x","")
            #Met les lettres en majuscule
            id = id.upper()
            #les id can doivent sur 4 digit minimum pour les id classiques
            if len(id) < 4 and trame["ext"] == '0':
                id = id.zfill(4)
            #les id can doivent sur 8 digit minimum pour les id extend
            if  trame["ext"] == 1:
                id = id.zfill(8)
            #conversion time
            time_ms = trame["time_ms"].split('.')[0]
            #car pas de us dans les data re√ßu par le log
            if "." in trame["time_ms"]:
                us = trame["time_ms"].split('.')[1]
            else :
                us = str(0)
            time_s = trame["time_s"]#[-5:] #garde uniquement les 5 premiers caracteres car sinon trop long
            #les ms et us doivent etre sur 3 digits
            if len(us) < 3:
                us = us.ljust(3,'0')
            if len(time_ms) < 3:
                time_ms = time_ms.zfill(3)
            #Pour avoir l'offset par rapport au d√©but de l'enregistrement
            #trame["start"] = 1768843635 #-------------test
            ms_int = int(time_s) - int(trame["start"])
            ms = str(ms_int) + time_ms #concatene le temps en s avec le temps en ms pour epochtime en ms
            
            #les data doivent √™tre en hexa
            data = []  #data2 pour inviter interference avec autres fonctions
            for j in range (0,trame['dlc']):
                data.append(str(hex(trame["data"][j])))
                data[j] = data[j].replace("0x","")
                data[j] = data[j].upper()
                if len(data[j]) < 2:
                    data[j] = data[j].zfill(2)
            
            #Mise en forme data en fonction du DLC
            data_to_write =""
            for k in range (0,trame["dlc"]):
                data_to_write = data_to_write + data[k] + " "

            #ecriture data
            f.write("\n" + (6-len(str(i+1)))*" " + str(i+1) + ")" +(10-len(ms))*" " + ms
            + "."+ us + " " + "1" + 2*" " + "Rx" + (12-len(id)) * " " 
            + id + " " + "_" + 2*" " + str(trame["dlc"]) + 4*" " + data_to_write)
            #+ " Data_sorter: " + trame["data_sorter"])  #Only for debug
            i+=1
    print("TRC debug saved ‚úÖ")

# Save any struct as json file
def save_json(struct):
    # R√©cup√©ration de la date et de l'heure actuelle pour le nom
    now = datetime.datetime.now()
    now_str = now.strftime("%d-%m-%Y_%H-%M-%S")
    # Get current directory
    current_dir = os.getcwd()
    # Set subdirectory
    sub_dir = "test"
    # Create the file name
    file_name =  os.path.join(current_dir,sub_dir,"json_" + now_str + ".json")
    # Enregistrez la structure dans un fichier texte au format JSON
    with open(file_name, 'w') as fichier:
        json.dump(struct, fichier, indent=1)#, sort_keys=True)
        print("Saved ‚úÖ")

# Save any struct as json file, with data hexa
def save_json_hex(struct):
    # R√©cup√©ration de la date et de l'heure actuelle pour le nom
    now = datetime.datetime.now()
    now_str = now.strftime("%d-%m-%Y_%H-%M-%S")
    # Get current directory
    current_dir = os.getcwd()
    # Set subdirectory
    sub_dir = "test"
    # Create the file name
    file_name =  os.path.join(current_dir,sub_dir,"json_" + now_str + ".json")
    # Convertit les trames en hexa
    for i,msg in enumerate(struct):
        struct[i]["hex"] = str([hex(entier) for entier in msg["data"]])
    # Enregistrez la structure dans un fichier texte au format JSON
    with open(file_name, 'w') as fichier:
        json.dump(struct, fichier, indent=1)#, sort_keys=True)
        print("Saved ‚úÖ")
    


def save_json_history(struct):
    for msg in struct:
        if msg !='NEW PACKET':
            msg["hex"] = str([hex(x) for x in msg["data"]])
    # R√©cup√©ration de la date et de l'heure actuelle pour le nom
    now = datetime.datetime.now()
    now_str = now.strftime("%d-%m-%Y_%H-%M-%S")
    # Get current directory
    current_dir = os.getcwd()
    # Set subdirectory
    sub_dir = "test"
    # Create the file name
    file_name =  os.path.join(current_dir,sub_dir,"json_hist_" + now_str + ".json")
    # Enregistrez la structure dans un fichier texte au format JSON
    with open(file_name, 'w') as fichier:
        json.dump(struct, fichier, indent=1)#, sort_keys=True)
        print("Saved ‚úÖ")

# Lis la trame, la d√©code et d√©concat√®ne, la partie data est d√©cod√© mais pas d√©compress√©
# UDH
def msg_mqtt_UDH(payload):
    global readed_dbc # objet dbc
    global cpt_rank # compteur de rang (num√©ro paquet)
    global start_time # epoch du d√©but de l'enregistrement
    global msg_UDH # datas lues
    global payload_history
    global t_last_receive
    global point_display
    global lock_list_data # s√©maphore pour acc√©der √† la liste des datas
    global history_msg_UDH # debug only
    global compteur_header_prev #m pr√©c√©dente valeur du compteur du header du paquet mqtt
    global compteur_header_history # debug les trames perdues

    # Raz list de message re√ßu
    msg_UDH = []

    # Fait varier le publish
    point_display = (point_display+1)%10
    print("Frame received" + point_display*".")

    #Get the current time when receiving data
    t_last_receive = time.time() 
    #Debug l'historique des valeurs re√ßues
    #payload_history.append(str(payload)) 
    payload_history.append(payload) # debug only
    
    # -------------- Debug payload --------------------
    #payload = b'\x11eM\xffI\x00\t\x00\x14\xff\x00\x89\xc0\xb5z\xf8\x1eH\n\x00&\x1f\x041\x8aS6\x0b\x007\xff|W\xfd\x84\x9a\xd6\x1b\xf4\x0c\x00B\x7fe\xfeY(\xd5\x18\x85\x11\x00F\x7f\x1a:R]\xc0\xd6\xef\x12\x00N\x7fB\x1b~\xaa\x0bIu\x13\x00T\x7f\xef<\xbe\x9c\xdaXY\x14\x00b\xff\xc4\x8a\x99\x0fjg\xf7\x1a\x16\x00b\x7f\x00iQ\xc8\xd2t\xdf\x17\x00e\x7f\xcam\x02\xed2\x9b\xe3\x18\x00k\xff\xc1\xbd\x83\xe4\x92\x9a\xc27\x1e\x00m\xffT\x04\x1f6\xfe\xed*\xff\x1f\x00{\xff1\xa64\xa0\xaa\x91Pd \x00\x88\xff\xa3,\x1ah\xc16\x1a\xac!\x00\x90\xffIX8\x9b\xedw\xe2g"\x00\x9a\xff\xb7\x8a\x84\xa4q\x95\xbaw#\x00\xaa\xff\x01\x8aCz\xbf\x0e\xec"'

    # ----- Decode the header ------
    # nb trame
    nb_trames = payload[0]
    # epoch s
    epoch_s = payload[1] << 24 | payload[2] << 16 | payload[3] << 8 | payload[4]
    # Get the first epoch as a start time
    if start_time == 0: #if=0 the var is unitialized
        start_time = epoch_s

    # Get compteur signal triangle et update graph
    compteur_header = payload[5]

    # Check compteur du header pour savoir si trame perdu
    #breakpoint()
    # Conditions initilaes
    #compteur_header_history.append(str(compteur_header) + " Error ? : "+  str(error_cpt))
    error_cpt = False
    if compteur_header_prev == -1 or compteur_header == compteur_header_prev + 1 or (compteur_header == 0 and compteur_header_prev==255):
        #print("OK ‚úÖ")
        compteur_header_history.append(compteur_header)
    else:
        error_cpt = True
        #print("ERROR ‚ùå")
        #print(" PREV : " + str(compteur_header_prev) + "   NEW : " + str(compteur_header))
        compteur_header_history.append(str(compteur_header) + " ERROR ")
    compteur_header_prev = compteur_header

    # Update rank / ordre de r√©ception
    cpt_rank += 1
    
    # ----- Decode the frame ------
    # init
    id_compressed = 0 # d√©claration et RAZ
    # nombre de 1 dans data_sorter
    nb_un = 0 
    offset = 0
    msg_can_data =[]
    
    #r√©cup√®re les infos par id
    for i in range(0,nb_trames):
        #recup le data sorter
        data_sorter_int =  payload[9+offset]
        #converti en string de binaire
        data_sorter = str(bin(data_sorter_int)[2:])
        #trouverle nombre de 1 dans le data sorter
        nb_un = 0
        for num in data_sorter:
            if num == '1':
                nb_un += 1
        #recup l'id compress√©
        id_compressed =  str(payload[6+offset])

        #parcour le fichier dbc pour le d√©coder
        for signal in readed_dbc.trames:
            #recup les infos associ√©s √† l'id compress√©
            if id_compressed == signal.id_compressed:
                msg_can_id = signal.id
                msg_can_dlc = signal.dlc
                msg_can_ext = str(int(signal.ext))
                msg_can_ts_ms = payload[7+offset] << 8 | payload[8+offset]
                #breakpoint() 
                #init
                msg_can_data =[]
                #r√©cup√®re les datas de sortie
                for j in range (0,nb_un):
                    msg_can_data.append(payload[10+offset+j])

                #Ajout structure
                msg_can = {'cpt_h': compteur_header,'cycle':signal.cycle,'error':False,
                           'rank':cpt_rank,'tri': epoch_s*1000+msg_can_ts_ms, 'start':start_time,
                           'id': msg_can_id, 'dlc': msg_can_dlc, 'ext': msg_can_ext,
                           'data_sorter': data_sorter.zfill(8), 'data': msg_can_data, "hex":None,
                           "prev":None, 'data_received': None,
                           'time_s': str(epoch_s), 'time_ms': str(msg_can_ts_ms)}
                msg_UDH.append(msg_can)

                # Bloque la s√©maphore
                #with lock_list_data:
                #print("Locked by MQTT Reading üîí \n")
                # Ajout √† la liste UDH
                list_msg_UDH.append(msg_can)
                history_msg_UDH.append(msg_can)
                #breakpoint()
                #pass
                #print("Unlocked by MQTT Reading ‚úÖ \n")
                break
                #breakpoint() 
        
        #maj cpt_rank 
        #cpt_rank += 1 
        
        #mise en m√©moire nb_un
        offset += nb_un + 4 
    # D√©compression
    #uncompress_mqtt_msg()
    # Allow to debug inside this callback
    # Normal breakpoints doesn't work
    #breakpoint() 
    pass

#Reconstitue la partie data par rapport au data sorter
def build_uncompress_data(msg_unconcatened,prev_msg,real_sorter):
    data_uncompressed = []
    #breakpoint() 
    j = 0
    for k,octet in enumerate(real_sorter):
        if octet == "0":
            data_uncompressed.append(prev_msg['data'][k])
        else:
            data_uncompressed.append(msg_unconcatened['data'][j])
            j += 1
    msg_unconcatened['data'] = data_uncompressed
    return  msg_unconcatened

# D√©compresse la partie data des messages mqtt
def uncompress_mqtt_msg(list_to_uncompress):
    # Debug error
    global cpt_error 
     # ----------------- init var --------------------
    #list msg uncompressed
    list_msg_uncompressed = []
    # Parcour la liste de msg re√ßu
    for msg_unconcatened in list_to_uncompress:
        # Message d√©compress√©
        msg_uncompressed = []
        dlc = msg_unconcatened['dlc']
        sorter = msg_unconcatened['data_sorter']
        # C'est le datasorter r√©el (ex: 111 au lieu de 0000 0011 si dlc=3)
        real_sorter = sorter[-dlc:]
        id = msg_unconcatened['id']
        cpt = msg_unconcatened['cpt_h']
        # Cherche le msg pr√©c√©dent pour d√©compresser la partie data
        # Flag pour savoir si des datas pr√©c√©dente exits pour l'id
        prev_data_exist = False
        for i, prev_msg in enumerate (prev_msg_per_id):
            # Check si des datas on d√©j√† √©tait lues pour cette id 
            # Si existe alors d√©compression
            if id == prev_msg_per_id[i]["id"]: 
                # Check si il manque une trame
                prev_cpt = prev_msg['cpt_h']
                prev_error = prev_msg['error']
                prev_data_exist = True
                ############### generate error ##############
                # cpt_error +=1
                # if cpt_error ==95 and real_sorter != dlc*'1':
                #     cpt = prev_cpt
                # if cpt_error == 992 and real_sorter != dlc*'1':
                #     cpt = prev_cpt
                #############################################
                # cpt n'est pas √©gal √† prev_cpt + 1 et la paire (prev_cpt, cpt) n'est pas √©gale √† (255, 0)
                # prev_error est vrai et pas de trame de synchro
                if cpt != prev_cpt +1 and not (prev_cpt == 255 and cpt==0) or prev_error and not (real_sorter == dlc*'1'):
                    # indication de l'erreur dans la struct du msg
                    msg_uncompressed=msg_unconcatened
                    #debug only
                    msg_uncompressed['error'] = True
                    # Mise en m√©moire de l'√©tat
                    prev_msg_per_id[i] = msg_uncompressed
                    # debug 
                    print("----------------------------------------------------------")
                    print("error, missing data")
                    print(cpt)
                    print(prev_cpt)
                    print(id)
                    print(msg_unconcatened)
                    print("----------------------------------------------------------")
                    break
                msg_uncompressed = build_uncompress_data(msg_unconcatened,prev_msg_per_id[i],real_sorter)
                #debug only
                debug_history.append(msg_uncompressed)
                # Mise en m√©moire de l'√©tat
                prev_msg_per_id[i] = msg_uncompressed
                break

        if prev_data_exist == False:
            # si existeet et que le data_sorter n'est pas tout √† 1 en fonction du DLC (synchro)
            if real_sorter == dlc*"1":
                # pas besoin de d√©compresser car d√©j√† fait
                prev_msg_per_id.append(msg_unconcatened)
                msg_uncompressed = msg_unconcatened
            else:
                print("Error d√©compression")
        list_msg_uncompressed.append(msg_uncompressed)
        msg_uncompressed_history.append(msg_uncompressed)
    return list_msg_uncompressed

# Save the payload history as json file
def save_payload_history():
    print("Json saving ‚åö") 
    global payload_history
     # R√©cup√©ration de la date et de l'heure actuelle pour le nom
    now = datetime.datetime.now()
    now_str = now.strftime("%d-%m-%Y_%H-%M-%S")
    # Get current directory
    current_dir = os.getcwd()
    # Set subdirectory
    sub_dir = "test"
    # Create the file name
    file_name =  os.path.join(current_dir,sub_dir,"payload_" + now_str + ".json")
    # Enregistrez la structure dans un fichier texte au format JSON
    with open(file_name, 'w') as fichier:
        json.dump(payload_history, fichier, indent=1)#, sort_keys=True)
    print("Json saved ‚úÖ")

# Callback when the subscribed topic receives a message
# Launch frames_received
def on_message_received(topic, payload, dup, qos, retain, **kwargs):
    msg_mqtt_UDH(payload)

# Read DBC and extract values
def read_dbc():
    #Cr√©er l'handler pour les trames
    trames_dbc = TramesDBC()
    # Name Init
    #name_dbc =  "DBC_VCU_V2.dbc"
    name_dbc = "D:\\10_Gaetan\\280_Lutece\\10_Test_BMS\\10_LabView\\DBC_VCU_V2.dbc"
    #handleur dbc (lis le dbc)
    database = cantools.database.load_file(name_dbc)
    #R√©cup√©ration des informations du dbc et maj des objets
    for index, can_msg in enumerate(database.messages):
        name = can_msg.name
        id_can_decimal = str(can_msg.frame_id)
        id_can_hexa = hex(int(id_can_decimal))[2:].upper()
        id_can_hexa = id_can_hexa.zfill(4)
        dlc = can_msg.length
        ext = can_msg.is_extended_frame
        id_compressed = str(index)
        can_index = 1
        cycle = can_msg.cycle_time
        #maj des objets
        trame_dbc = TrameDBC(name, id_can_decimal, id_can_hexa ,dlc,ext,
                      id_compressed,can_index,cycle)
        #ajout au handler
        trames_dbc.add_trame(trame_dbc)
    return trames_dbc

# Ordering data and give it to labview in real time
def get_data_lbv_rt():
    # Liste des messages re√ßu
    global list_msg_UDH
    global q_for_lbv
    global stop_all
    global debug_error
    global debug_valid
    while not stop_all:
        to_send = []
        to_send_sorted = []
        to_send_uncompressed = []
        # liste qui sert de tampon pour maj la liste associ√©e
        futur_list_msg_UDH =[]
        if list_msg_UDH: # check si liste est non vide
            # Fait une copie de la liste de messages
            list_msg_UDH_copy = deepcopy(list_msg_UDH)
            # Temps r√©el
            actual_time = time.time()*1000
            # Fabrique un retard de 5 sec au first shot
            if actual_time - list_msg_UDH_copy[0]["start"]*1000 > 5:
                for msg in list_msg_UDH_copy: # parcour les datas
                    if msg["tri"] < actual_time - READ_DELAY*1000: #check si data avec 5s de retard
                        to_send.append(msg)
                    else:
                        # Ajoute les datas √† l'historique (debug)
                        futur_list_msg_UDH.append(msg)
                # maj de ma list des msg
                with lock_list_data:
                    list_msg_UDH = futur_list_msg_UDH
            # Ordonancement des datas pour labview
            to_send_sorted = sorted(to_send,key=itemgetter('tri'))
            to_send_uncompressed = uncompress_mqtt_msg(to_send_sorted)
            #msg_lbv_history.append("NEW PACKET") # debug
            if to_send_uncompressed:
                msg_lbv_history.extend(to_send_uncompressed)
                for msg in to_send_uncompressed:
                    #Check si erreur dans msg
                    #debug_history.append(msg)
                    if msg["error"] == False:
                        q_for_lbv.put(msg)
                        debug_valid.append(msg)
                    else:
                        debug_error.append(msg)

        #Dors 5s
        time.sleep(READ_DELAY)
    # termine la t√¢che mqtt si on a le flag stop
    mqtt_connection.disconnect()

# Lance le serveur TCP et attend la connexion avec Labview
def connect_tcp():
    global serveur_socket
    global client_socket
    # Cr√©er un socket TCP/IP pour le serveur
    serveur_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    # Lier le socket √† une adresse et un port
    adresse_serveur = ('localhost', 12345)
    serveur_socket.bind(adresse_serveur)
    # Mettre le socket en mode √©coute
    serveur_socket.listen(1)
    print(f"Le serveur √©coute sur {adresse_serveur[0]}:{adresse_serveur[1]} ‚åö")
    # Attendre une connexion
    client_socket, client_address = serveur_socket.accept()
    print("Labview connected ‚úÖ")

# Envoi les datas a Labview via TCP
def write_tcp():
    global q_for_lbv
    global serveur_socket
    global client_socket
    global sleep_time_factor
    global t_prev_time
    sleep_time_factor = 0.8
    # Launch TCP server
    try:
        connect_tcp()
    except:
        print("Error TCP server ‚ùå")
    while not serveur_socket._closed:
        try :
            # debug only
            # save = False
            # if save  == True:
            #     save_trc_debug()
            # Check si msg dans la queue
            if not q_for_lbv.empty():
                # R√©cup√®re la taille de la queue
                nb_msg = q_for_lbv.qsize()
                nb_msg_lbv = nb_msg.to_bytes(4,"big")
                # R√©cup√®re un √©l√©ment de la queue
                return_lbv = q_for_lbv.get()
                # calcul le temps 
                now = return_lbv["tri"]
                if t_prev_time:
                    sleep_time = abs(int(now - t_prev_time)/1000)
                else:
                    sleep_time=0
                t_prev_time = now
                # Conversion ID
                id = int(return_lbv["id"])
                id = id.to_bytes(4,"big")
                # Conversion Data sur 8 octets
                data = return_lbv["data"] + [0]*(8-len(return_lbv["data"]))
                data_bytes = bytes(data)
                message = data_bytes
                # Envoi l'id puis le mssage puis l'√©tat de la fifo
                client_socket.sendall(id)
                client_socket.sendall(message)
                client_socket.sendall(nb_msg_lbv)
                # Debug only 
                #debug_history.append(return_lbv)
                # Si delta entre trame <3ms alors on le n√©glige
                if sleep_time>=0.003:
                    time.sleep(sleep_time)
            else:
                # sleep 10 ms si pas de trame
                time.sleep(0.01) 
                
        except Exception as e:
            print(e)
            raise # Montre o√π est l'erreur
            # Fermer les sockets si erreur de connexion
            client_socket.close()
            serveur_socket.close()
        

# Print la payload pour pouvoir la r√©-utiliser
def debug_payload():
    global payload_history
    # Exemple de tableau (liste en Python)
    mon_tableau = payload_history

    # Nom du fichier dans lequel enregistrer le tableau
    nom_fichier = "mon_fichier.txt"

    # Ouverture du fichier en mode √©criture
    with open(nom_fichier, "w") as fichier:
        # √âcriture de chaque √©l√©ment du tableau dans le fichier
        for element in mon_tableau:
            fichier.write(str(element) + "\n")
# ---------------------------MAIN-------------------------------------

def debug_calcul_time():
    global debug_history
    total = 0
    for msg in debug_history:
        total += msg["exe_time"]
    print(total) 


# Read DBC and extract values
global readed_dbc
readed_dbc = read_dbc()

# Cpt de num√©ro de paquet (1 paquet = 1 trame mqtt constitu√© de plusieurs trames CAN concat√©n√©es )
global cpt_rank
cpt_rank = 0

global compteur_header_prev
compteur_header_prev = -1
global compteur_header_history
compteur_header_history = []

global start_time
start_time = 0

# Stock les trames uncocancat√©n√©es et d√©compress√©es sauf partie data du dernier packet re√ßu d'MQTT 
# UDH : Uncon, D: Decompress, H: Half
global msg_UDH
msg_UDH = []

# Stock toutes les trames lues et UDH (Debug only)
global history_msg_UDH
history_msg_UDH = []

# Stock toutes les trames udh re√ßues
# Toutes les 5s data avec √¢ge>5s supprim√©es
global list_msg_UDH
list_msg_UDH = []
READ_DELAY = 2

# List every Publiseh messga received
global payload_history
payload_history = []

# Temps de la deni√®re r√©cpetion d'une publication
global t_last_receive
t_last_receive = time.time()

# compteur permettnat de faire varier le print des published pour voir si on ublie toujours
global point_display 
point_display = 0

# Var stockant le temps pr√©c√©dent (Thread for LBV)
global t_prev_time
t_prev_time = 0

# handler connexion tcp
global serveur_socket
serveur_socket = None

# Previous msg per id, fausse premir√®e data pour pouvoir boucler quand vide
prev_msg_per_id=[{"id":None,"cpt_sync":0}]

#for debug only
global debug_history
debug_history=[]
msg_uncompressed_history = []
global msg_uncompress_history_sorted
msg_uncompress_history_sorted = []
#for debug only
msg_lbv_history = []
msg_tri_history_unsorted = []
msg_tri_history_sorted = []
# Debug error
global cpt_error 
cpt_error = 0
global debug_error
debug_error = []
global debug_valid
debug_valid= []

# ------------ Init Thread ------------------------
# Cr√©√© s√©maphore (lock en python) pour partager la liste de lecture des datas
global lock_list_data
lock_list_data = threading.Lock()

global q_for_lbv
q_for_lbv = Queue()

# flag pour √©teindre les threads
global stop_all
stop_all = False

# temps de sommeil de la focntion for_lbv
global sleep_time
sleep_time = 0

# flag nouvelle queue lbv
global f_new_queue
f_new_queue = False

#Thread pour ordonancer et renvoyer les datas a LBV
thread_get_data = threading.Thread(target=get_data_lbv_rt)
thread_tcp = threading.Thread(target=write_tcp)
#thread_data_from_lbv = threading.Thread(target=data_from_lbv)



# q_msg_uncompressed = Queue()
# q_msg_sorted = Queue()

# ----------------------Init MQTT--------------------------
ENDPOINT = "a1xvyu1lci6ieh-ats.iot.eu-west-3.amazonaws.com"
CLIENT_ID = "subscribe_AwsPy"
# PATH_TO_CERTIFICATE = "certificates/1334cda3fa4d4a36ea0a8a7755bdbba76db34541e9f23f89388c83d357d46527-certificate.pem.crt"
# PATH_TO_PRIVATE_KEY = "certificates/1334cda3fa4d4a36ea0a8a7755bdbba76db34541e9f23f89388c83d357d46527-private.pem.key"
# PATH_TO_AMAZON_ROOT_CA_1 = "certificates/root.pem"
PATH_TO_CERTIFICATE = "D:\\10_Gaetan\\280_Lutece\\10_Test_BMS\\10_LabView\\certificates\\1334cda3fa4d4a36ea0a8a7755bdbba76db34541e9f23f89388c83d357d46527-certificate.pem.crt"
PATH_TO_PRIVATE_KEY = "D:\\10_Gaetan\\280_Lutece\\10_Test_BMS\\10_LabView\\certificates\\1334cda3fa4d4a36ea0a8a7755bdbba76db34541e9f23f89388c83d357d46527-private.pem.key"
PATH_TO_AMAZON_ROOT_CA_1 = "D:\\10_Gaetan\\280_Lutece\\10_Test_BMS\\10_LabView\\certificates\\root.pem"
TOPIC = "pyAws"
#TOPIC = "bateau/frames"
SAVE_ACTIVATED = False #active ou d√©sactive la sauvegarde des datas mqtt
SAVE_TIMER = 10 #Timer pour sauvegarde data mqtt  6000 = 1 min

# Spin up resources
event_loop_group = io.EventLoopGroup(1)
host_resolver = io.DefaultHostResolver(event_loop_group)
client_bootstrap = io.ClientBootstrap(event_loop_group, host_resolver)

# Create the MQTT client
mqtt_connection = mqtt_connection_builder.mtls_from_path(
            endpoint=ENDPOINT,
            cert_filepath=PATH_TO_CERTIFICATE,
            pri_key_filepath=PATH_TO_PRIVATE_KEY,
            client_bootstrap=client_bootstrap,
            ca_filepath=PATH_TO_AMAZON_ROOT_CA_1,
            client_id=CLIENT_ID,
            clean_session=False,
            keep_alive_secs=6
            )
print("Connecting ‚åö")
# Make the connect() call
connect_future = mqtt_connection.connect()
# Future.result() waits until a result is available
connect_future.result()
print("Connected ‚úÖ")

# Subscribe to the topic
print("Subscribing ‚åö")
subscribe_future, packet_id = mqtt_connection.subscribe(
        topic=TOPIC,
        #qos=mqtt.QoS.AT_LEAST_ONCE,
        qos=mqtt.QoS.AT_MOST_ONCE,
        callback=on_message_received)
subscribe_result = subscribe_future.result()

print("Subscribed ‚úÖ")

# Launch TCP server
#connect_tcp()

# Lance les threads
thread_get_data.start()  
thread_tcp.start()
#thread_data_from_lbv.start()


